{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('./entailment_bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cmaspi/.local/share/virtualenvs/multistep-reasoning-hBGzQBr9/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "from utils.nlp_agent import MultiAngleModel, NlpAgent\n",
    "\n",
    "ew_model = MultiAngleModel(model_path=\"allenai/entailer-large\", cuda_devices=None)\n",
    "prover = NlpAgent(model=ew_model, default_outputs=\"proof\")\n",
    "entail_verifier = NlpAgent(model=ew_model, default_outputs=[\"implied\"], default_options={\"explicit_outputs\": ['true', 'false']})\n",
    "hyp_verifier = NlpAgent(model=ew_model, default_outputs=[\"valid\"], default_options={\"explicit_outputs\": ['true', 'false']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_raw': '$proof$ ; $hypothesis$ = Covid-19 is not a deadly disease', 'output_raw_list': ['$proof$ = [PREMISE] Covid-19 is a kind of disease. [PREMISE] Covid-19 is not a deadly disease.', '$proof$ = [PREMISE] Covid-19 is a kind of disease. [PREMISE] Covid-19 is not a deadly disease.', '$proof$ = [PREMISE] Covid-19 is not a deadly disease. [PREMISE] Covid-19 is a kind of virus.', '$proof$ = [PREMISE] Covid-19 is not a deadly disease. [PREMISE] Covid-19 is a kind of virus.', '$proof$ = [PREMISE] Covid-19 is not a deadly disease. [PREMISE] Covid-19 is a kind of virus.', '$proof$ = [PREMISE] Covid-19 is a kind of disease. [PREMISE] Covid-19 is not a deadly disease.', '$proof$ = [PREMISE] Covid-19 is a kind of disease. [PREMISE] Covid-19 is not a deadly disease.', '$proof$ = [PREMISE] Covid-19 is a kind of disease. [PREMISE] Covid-19 is not a deadly disease.', '$proof$ = [PREMISE] Covid-19 is not a deadly disease. [PREMISE] Covid-19 is a kind of virus.', '$proof$ = [PREMISE] Covid-19 is a kind of disease. [PREMISE] Covid-19 is not a deadly disease.']}\n",
      "torch.Size([10, 36])\n",
      "tensor([[ 0.0199,  0.0065,  0.0097,  ...,  0.0077,  0.0039, -0.0022],\n",
      "        [ 0.0199,  0.0065,  0.0097,  ...,  0.0077,  0.0039, -0.0022],\n",
      "        [ 0.0159,  0.0042,  0.0078,  ...,  0.0098,  0.0044, -0.0043],\n",
      "        ...,\n",
      "        [ 0.0199,  0.0065,  0.0097,  ...,  0.0077,  0.0039, -0.0022],\n",
      "        [ 0.0159,  0.0042,  0.0078,  ...,  0.0098,  0.0044, -0.0043],\n",
      "        [ 0.0199,  0.0065,  0.0097,  ...,  0.0077,  0.0039, -0.0022]])\n"
     ]
    }
   ],
   "source": [
    "# Try to prove a hypothesis\n",
    "hyp = \"Covid-19 is not a deadly disease\"\n",
    "proof, score = prover({\"hypothesis\": hyp})\n",
    "premises = [x.strip() for x in proof.split(\"[PREMISE]\") if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_raw': '$valid$ ; $hypothesis$ = Covid-19 is not a deadly disease', 'output_raw_list': ['$valid$ = false', '$valid$ = false', '$valid$ = false', '$valid$ = false', '$valid$ = false', '$valid$ = false', '$valid$ = false', '$valid$ = false', '$valid$ = false', '$valid$ = false']}\n",
      "torch.Size([10, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'valid': 'false', 'output_prob': 0.9864044189453125}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp_verifier({\"hypothesis\": hyp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[PREMISE] Covid-19 is a kind of disease. [PREMISE] Covid-19 is not a deadly disease.',\n",
       " tensor(-8.1763))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proof, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ew_model.model['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mgeneration_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerationConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mlogits_processor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogitsProcessorList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstopping_criteria\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopping_criteria\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStoppingCriteriaList\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprefix_allowed_tokens_fn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0msynced_gpus\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0massistant_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PreTrainedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BaseStreamer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnegative_prompt_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateBeamDecoderOnlyOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateBeamEncoderDecoderOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Generates sequences of token ids for models with a language modeling head.\n",
      "\n",
      "<Tip warning={true}>\n",
      "\n",
      "Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "\n",
      "For an overview of generation strategies and code examples, check out the [following\n",
      "guide](../generation_strategies).\n",
      "\n",
      "</Tip>\n",
      "\n",
      "Parameters:\n",
      "    inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "        The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "        method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "        should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "        `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "    generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "        The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "        passed to generate matching the attributes of `generation_config` will override them. If\n",
      "        `generation_config` is not provided, the default will be used, which has the following loading\n",
      "        priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "        configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "        default values, whose documentation should be checked to parameterize generation.\n",
      "    logits_processor (`LogitsProcessorList`, *optional*):\n",
      "        Custom logits processors that complement the default logits processors built from arguments and\n",
      "        generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "        generation config an error is thrown. This feature is intended for advanced users.\n",
      "    stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "        Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "        generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "        generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "        sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "        intended for advanced users.\n",
      "    prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "        If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "        provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "        `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "        on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "        for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "        Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "    synced_gpus (`bool`, *optional*):\n",
      "        Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "        `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "        generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "    assistant_model (`PreTrainedModel`, *optional*):\n",
      "        An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "        same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "        is much faster than running generation with the model you're calling generate from. As such, the\n",
      "        assistant model should be much smaller.\n",
      "    streamer (`BaseStreamer`, *optional*):\n",
      "        Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "        through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "    negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "        size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "    negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "        Attention_mask for `negative_prompt_ids`.\n",
      "    kwargs (`Dict[str, Any]`, *optional*):\n",
      "        Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "        forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "        specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "\n",
      "Return:\n",
      "    [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "    or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`.\n",
      "\n",
      "        If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "        [`~utils.ModelOutput`] types are:\n",
      "\n",
      "            - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "            - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "\n",
      "        If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "        [`~utils.ModelOutput`] types are:\n",
      "\n",
      "            - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "            - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "\u001b[0;31mFile:\u001b[0m      ~/.local/share/virtualenvs/multistep-reasoning-hBGzQBr9/lib/python3.10/site-packages/transformers/generation/utils.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "model.generate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
